{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN0JncwKZtrBWtLLVBXPTC5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JUANCHOLOCO/LanGrandTaller1/blob/main/colabLaboratorio1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalaci贸n\n",
        "%%capture\n",
        "# Instalar paquetes necesarios (ejecutar en celda separada en Colab)\n",
        "!pip install -U langchain-google-genai langgraph langchain-core langchain-community python-dotenv"
      ],
      "metadata": {
        "id": "YyPvw_32umeV"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# C贸digo: Agente LangGraph para dominio Java (ajustado)\n",
        "# NOTA: instala dependencias en tu entorno antes de ejecutar.\n",
        "import os\n",
        "from typing import TypedDict, Annotated, Sequence, Optional, Dict\n",
        "import operator\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "# LLM: imports (ajusta seg煤n librer铆a exacta instalada)\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# LLM: imports (ajusta seg煤n librer铆a exacta instalada)\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI"
      ],
      "metadata": {
        "id": "hGYA7sumuq3F"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# TypedDict (esquema de estado)\n",
        "# -------------------------\n",
        "class AgentState(TypedDict):\n",
        "    messages: Annotated[Sequence[dict], operator.add]\n",
        "    analysis_results: Dict[str, object]\n",
        "    retries: int\n",
        "    metadata: Optional[Dict[str, object]]\n"
      ],
      "metadata": {
        "id": "gnooZ6G-wDou"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "# 2. INICIALIZAR AGENTES/LLMs\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = userdata.get('GOOGLE_API_KEY')\n",
        "# -------------------------\n",
        "#  CORRECCIN: Instancia real del modelo Gemini 2.5 Flash\n",
        "# -------------------------\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    temperature=0.3,\n",
        "    max_output_tokens=1000,\n",
        "    google_api_key='GOOGLE_API_KEY'\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "48CHapyyVkZg"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# LLM wrapper: intenta varios m茅todos de invocaci贸n para robustez\n",
        "# -------------------------\n",
        "def send_messages_to_llm(llm_instance, messages):\n",
        "    \"\"\"\n",
        "    Intentar谩 invocar el LLM con la lista 'messages' (lista de dicts role/content).\n",
        "    Soporta varios m茅todos (invoke, __call__, generate, predict_messages).\n",
        "    \"\"\"\n",
        "    # Si el LLM provee 'invoke' (tu versi贸n original)\n",
        "    try:\n",
        "        if hasattr(llm_instance, \"invoke\"):\n",
        "            return llm_instance.invoke(messages)\n",
        "        # callable LLM (p.ej. langchain ChatModels suelen ser callable)\n",
        "        if callable(llm_instance):\n",
        "            return llm_instance(messages)\n",
        "        # m茅todo 'generate' o 'predict_messages'\n",
        "        if hasattr(llm_instance, \"generate\"):\n",
        "            return llm_instance.generate(messages)\n",
        "        if hasattr(llm_instance, \"predict_messages\"):\n",
        "            return llm_instance.predict_messages(messages)\n",
        "    except Exception as e:\n",
        "        # No abortar; devolver una estructura coherente para manejo de fallback\n",
        "        return type(\"LLMResponse\", (), {\"content\": f\"LLM invocation error: {e}\"})\n",
        "\n",
        "    raise RuntimeError(\"LLM instance no tiene m茅todo compatible (invoke/call/generate/predict_messages)\")\n"
      ],
      "metadata": {
        "id": "YiEgy5DNwwPe"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Nodos del grafo\n",
        "# -------------------------\n",
        "def analyze_input_node(state: AgentState):\n",
        "    \"\"\"\n",
        "    Analiza la entrada: llama al LLM con un prompt de 'analista experto'.\n",
        "    Actualiza analysis_results y current_step.\n",
        "    \"\"\"\n",
        "    messages = list(state[\"messages\"])  # copy-safe\n",
        "    system_prompt = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"Eres un analista experto en backend Java. Extrae intenci贸n (debug/config/review), \"\n",
        "            \"confidence estimada y detalles (stacktrace, componentes involucrados).\"\n",
        "        )\n",
        "    }\n",
        "    gemini_messages = [system_prompt] + messages\n",
        "\n",
        "    response = send_messages_to_llm(llm, gemini_messages)\n",
        "    content = getattr(response, \"content\", str(response))\n",
        "\n",
        "    # Aqu铆 podr铆as parsear JSON si el LLM devuelve un JSON estructurado\n",
        "    # Para el ejemplo, hacemos parseo simple heur铆stico:\n",
        "    # (en producci贸n: usar schema validation / tools como pydantic)\n",
        "    analysis_results = {\n",
        "        \"intent\": \"unknown\",\n",
        "        \"confidence\": 0.5,\n",
        "        \"details\": content\n",
        "    }\n",
        "\n",
        "    # heur铆stica simple: si aparece 'JdbcCursorItemReader' considerarlo debug_spring_batch\n",
        "    if \"JdbcCursorItemReader\" in content or \"cursor\" in content.lower():\n",
        "        analysis_results[\"intent\"] = \"debug_spring_batch\"\n",
        "        analysis_results[\"confidence\"] = 0.9\n",
        "    elif \"stacktrace\" in content.lower() or \"exception\" in content.lower():\n",
        "        analysis_results[\"intent\"] = \"debug\"\n",
        "        analysis_results[\"confidence\"] = 0.7\n",
        "\n",
        "    return {\n",
        "        \"analysis_results\": analysis_results\n",
        "    }"
      ],
      "metadata": {
        "id": "1iw3mmC7HlUk"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decide_node(state: AgentState):\n",
        "    \"\"\"\n",
        "    Nodo de decisi贸n. Dependiendo de confidence y reglas, indica la siguiente acci贸n.\n",
        "    Retorna current_step pero tambi茅n a trav茅s del grafo hacemos la transici贸n condicional.\n",
        "    \"\"\"\n",
        "    analysis = state[\"analysis_results\"]\n",
        "    confidence = float(analysis.get(\"confidence\", 0.0))\n",
        "    # umbral configurable: 0.70\n",
        "    threshold = 0.7\n",
        "    next_step = \"respond\" if confidence >= threshold else \"fallback\"\n",
        "    return {\"next_step_choice\": next_step}"
      ],
      "metadata": {
        "id": "kIFJehYyHvVi"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_response_node(state: AgentState):\n",
        "    \"\"\"\n",
        "    Genera la respuesta final (respuestas t茅cnicas con snippets).\n",
        "    Actualiza messages y current_step.\n",
        "    \"\"\"\n",
        "    messages = list(state[\"messages\"])\n",
        "    analysis = state[\"analysis_results\"]\n",
        "    system_prompt = {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            f\"Eres un asistente t茅cnico. Basado en este an谩lisis: intent={analysis.get('intent')}, \"\n",
        "            f\"confidence={analysis.get('confidence')}. Proporciona respuesta t茅cnica concisa y ejemplos.\"\n",
        "        )\n",
        "    }\n",
        "    gemini_messages = [system_prompt] + messages\n",
        "    response = send_messages_to_llm(llm, gemini_messages)\n",
        "    content = getattr(response, \"content\", str(response))\n",
        "\n",
        "    new_messages = messages + [{\"role\": \"assistant\", \"content\": content}]\n",
        "    return {\"messages\": new_messages}\n"
      ],
      "metadata": {
        "id": "Vu_Asi1mHwcr"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fallback_node(state: AgentState):\n",
        "    \"\"\"\n",
        "    Fallback: cuando confidence es baja o falta informaci贸n. Incrementa retries y pide m谩s datos.\n",
        "    \"\"\"\n",
        "    messages = list(state[\"messages\"])\n",
        "    retries = int(state.get(\"retries\", 0)) + 1\n",
        "    fallback_msg = (\n",
        "        \"No tengo suficiente informaci贸n confiable para dar un diagn贸stico preciso. \"\n",
        "        \"Por favor adjunta: logs (煤ltimas 200 l铆neas), stacktrace completo, versi贸n Java, \"\n",
        "        \"configuraci贸n del DataSource y del ItemReader. Mientras tanto, revisa: enable DEBUG logs, \"\n",
        "        \"thread dump y heap dump.\"\n",
        "    )\n",
        "    new_messages = messages + [{\"role\":\"assistant\",\"content\":fallback_msg}]\n",
        "    return {\"messages\": new_messages, \"retries\": retries}\n"
      ],
      "metadata": {
        "id": "iccQbUsIHzTr"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Wrappers para el grafo (funciones que LangGraph espera)\n",
        "# -------------------------\n",
        "def analyze_wrap(state):\n",
        "    return analyze_input_node(state)\n",
        "\n",
        "def decide_wrap(state):\n",
        "    return decide_node(state)\n",
        "\n",
        "def respond_wrap(state):\n",
        "    return generate_response_node(state)\n",
        "\n",
        "def fallback_wrap(state):\n",
        "    return fallback_node(state)\n"
      ],
      "metadata": {
        "id": "AHg6BGYpH4d6"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Construcci贸n del grafo\n",
        "# -------------------------\n",
        "workflow = StateGraph(AgentState)\n",
        "workflow.add_node(\"analyze\", analyze_wrap)\n",
        "workflow.add_node(\"decide\", decide_wrap)\n",
        "workflow.add_node(\"respond\", respond_wrap)\n",
        "workflow.add_node(\"fallback\", fallback_wrap)\n",
        "\n",
        "workflow.add_edge(START, \"analyze\")\n",
        "workflow.add_edge(\"analyze\", \"decide\")\n",
        "# Edges from decide are handled por la l贸gica de decisi贸n; aqu铆 a帽adimos ambos caminos\n",
        "workflow.add_edge(\"decide\", \"respond\")\n",
        "workflow.add_edge(\"decide\", \"fallback\")\n",
        "workflow.add_edge(\"respond\", END)\n",
        "workflow.add_edge(\"fallback\", END)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HnVs7ChH7mD",
        "outputId": "f1cde105-5495-4188-b1b9-ccc252e41371"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<langgraph.graph.state.StateGraph at 0x7dceecfc0e60>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Checkpointing y ejecuci贸n (ejemplo)\n",
        "# -------------------------\n",
        "memory = MemorySaver()\n",
        "app = workflow.compile(checkpointer=memory)\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"langgraph_java_backend_agent_v1\"}}\n",
        "initial_state: AgentState = {\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"Tengo este error org.springframework.dao.InvalidDataAccessResourceUsageException: Unexpected cursor position change. Uso JdbcCursorItemReader.\"}\n",
        "    ],\n",
        "    \"current_step\": \"initial\",\n",
        "    \"analysis_results\": {},\n",
        "    \"retries\": 0,\n",
        "    \"metadata\": {}\n",
        "}\n"
      ],
      "metadata": {
        "id": "bgR92RlvIDG6"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_result(result):\n",
        "    messages = result.get(\"messages\", [])\n",
        "\n",
        "    # Buscar el 煤ltimo mensaje del assistant\n",
        "    assistant_msgs = [m for m in messages if m.get(\"role\") == \"assistant\"]\n",
        "\n",
        "    print(\"\\n========== RESPUESTA FINAL ==========\\n\")\n",
        "\n",
        "    if assistant_msgs:\n",
        "        print(assistant_msgs[-1][\"content\"])\n",
        "    else:\n",
        "        print(\"锔 No se encontr贸 respuesta del asistente.\")\n",
        "\n",
        "    print(\"\\n=====================================\\n\")\n"
      ],
      "metadata": {
        "id": "PE94Ct8HSnPn"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_with_errors(result):\n",
        "    print(\"\\n========== RESULTADO ==========\\n\")\n",
        "\n",
        "    # Respuesta del asistente\n",
        "    assistant_msgs = [m for m in result.get(\"messages\", []) if m.get(\"role\") == \"assistant\"]\n",
        "\n",
        "    if assistant_msgs:\n",
        "        print(\" Respuesta del asistente:\\n\")\n",
        "        print(assistant_msgs[-1][\"content\"])\n",
        "    else:\n",
        "        print(\"锔 No hubo respuesta del asistente.\")\n",
        "\n",
        "    # Error del LLM\n",
        "    analysis = result.get(\"analysis_results\", {})\n",
        "    details = analysis.get(\"details\")\n",
        "\n",
        "    if details and \"API key not valid\" in details:\n",
        "        print(\"\\n ERROR DE CONFIGURACIN:\")\n",
        "        print(\"La API Key de Gemini no es v谩lida o no est谩 configurada.\")\n",
        "\n",
        "    print(\"\\n===============================\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cxjhvVgBTrUY"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecuta el grafo (si llm est谩 instanciado). Si no, el wrapper devolver谩 warning.\n",
        "if llm is None:\n",
        "    print(\"AVISO: llm es None. Instancia tu ChatGoogleGenerativeAI y asigna a 'llm' antes de invocar.\")\n",
        "else:\n",
        "    result = app.invoke(initial_state, config=config)\n",
        "    pretty_print_result(result)\n",
        "    pretty_print_with_errors(result)\n",
        "    print(\"Resultado final:\", result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2a-UsVfIEVS",
        "outputId": "5aef79d5-5786-4e4e-a797-ca2402d2b483"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========== RESPUESTA FINAL ==========\n",
            "\n",
            "LLM invocation error: Error calling model 'gemini-2.5-flash' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}\n",
            "\n",
            "=====================================\n",
            "\n",
            "\n",
            "========== RESULTADO ==========\n",
            "\n",
            " Respuesta del asistente:\n",
            "\n",
            "LLM invocation error: Error calling model 'gemini-2.5-flash' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}\n",
            "\n",
            " ERROR DE CONFIGURACIN:\n",
            "La API Key de Gemini no es v谩lida o no est谩 configurada.\n",
            "\n",
            "===============================\n",
            "\n",
            "Resultado final: {'messages': [{'role': 'user', 'content': 'Tengo este error org.springframework.dao.InvalidDataAccessResourceUsageException: Unexpected cursor position change. Uso JdbcCursorItemReader.'}, {'role': 'user', 'content': 'Tengo este error org.springframework.dao.InvalidDataAccessResourceUsageException: Unexpected cursor position change. Uso JdbcCursorItemReader.'}, {'role': 'assistant', 'content': 'No tengo suficiente informaci贸n confiable para dar un diagn贸stico preciso. Por favor adjunta: logs (煤ltimas 200 l铆neas), stacktrace completo, versi贸n Java, configuraci贸n del DataSource y del ItemReader. Mientras tanto, revisa: enable DEBUG logs, thread dump y heap dump.'}, {'role': 'user', 'content': 'Tengo este error org.springframework.dao.InvalidDataAccessResourceUsageException: Unexpected cursor position change. Uso JdbcCursorItemReader.'}, {'role': 'assistant', 'content': \"LLM invocation error: Error calling model 'gemini-2.5-flash' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}\"}], 'analysis_results': {'intent': 'unknown', 'confidence': 0.5, 'details': \"LLM invocation error: Error calling model 'gemini-2.5-flash' (INVALID_ARGUMENT): 400 INVALID_ARGUMENT. {'error': {'code': 400, 'message': 'API key not valid. Please pass a valid API key.', 'status': 'INVALID_ARGUMENT', 'details': [{'@type': 'type.googleapis.com/google.rpc.ErrorInfo', 'reason': 'API_KEY_INVALID', 'domain': 'googleapis.com', 'metadata': {'service': 'generativelanguage.googleapis.com'}}, {'@type': 'type.googleapis.com/google.rpc.LocalizedMessage', 'locale': 'en-US', 'message': 'API key not valid. Please pass a valid API key.'}]}}\"}, 'retries': 1, 'metadata': {}}\n"
          ]
        }
      ]
    }
  ]
}